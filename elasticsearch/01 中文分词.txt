配置ik插件
安装步骤：
网站地址为：https://github.com/medcl/elasticsearch-analysis-ik

注意,下载对应的版本:
IK version	ES version
master	1.5.0 -> master
1.4.0	1.6.0
1.3.0	1.5.0
1.2.9	1.4.0
1.2.8	1.3.2
1.2.7	1.2.1
1.2.6	1.0.0
1.2.5	0.90.2
1.2.3	0.90.2
1.2.0	0.90.0
1.1.3	0.20.2
1.1.2	0.19.x
1.0.0	0.16.2 -> 0.19.0

上传
put elasticsearch-analysis-ik-1.3.0.zip

2、解压, 编译
解压: 
unzip elasticsearch-analysis-ik-1.3.0.zip  
cd elasticsearch-analysis-ik-1.3.0
拷贝conf/ik目录到es的config目录下: 
cp -r config/ik  /etc/elasticsearch/
编译: 
mvn clean package

5,  执行安装命令
./plugin --install analysis-ik --url file:///tmp/elasticsearch-analysis-ik-1.3.0/target/releases/elasticsearch-analysis-ik-1.3.0.zip

6、在ES的配置文件中增加ik的配置，在最后增加：
vi /etc/elasticsearch/elasticsearch.yml
index:
  analysis:
    analyzer:
      ik:
          alias: [ik_analyzer]
          type: org.elasticsearch.index.analysis.IkAnalyzerProvider
      ik_max_word:
          type: ik
          use_smart: false
      ik_smart:
          type: ik
          use_smart: true

Or
index.analysis.analyzer.ik.type : "ik"

以上两种配置方式的区别：
1、第二种方式，只定义了一个名为 ik 的 analyzer，其 use_smart 采用默认值 false
2、第一种方式，定义了三个 analyzer，分别为：ik、ik_max_word、ik_smart，其中 ik_max_word 和 ik_smart 是基于 ik 这个 analyzer 定义的，并各自明确设置了 use_smart 的不同值。
3、其实，ik_max_word 等同于 ik。ik_max_word 会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合；而 ik_smart 会做最粗粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,国歌”。

因此，建议，在设置 mapping 时，用 ik 这个 analyzer，以尽可能地被搜索条件匹配到。
不过，如果你想将 /index_name/_analyze 这个 RESTful API 做为分词器用，用来提取某段文字中的主题词，则建议使用 ik_smart 这个 analyzer：

线上配置：
index:
  analysis:
    analyzer:
      ik:
          alias: [news_analyzer_ik,ik_analyzer]
          type: org.elasticsearch.index.analysis.IkAnalyzerProvider

index.analysis.analyzer.default.type : "ik"


7、重新启动elasticsearch服务，这样就完成配置了，收入命令：

检查启动日志
tail -f /var/log/elasticsearch/es_test_cluster.log
看到以下几行表示ik配置成功
loaded [analysis-ik, marvel], sites [marvel, head]
[Dict Loading]ik/custom/mydict.dic
[Dict Loading]ik/custom/single_word_low_freq.dic
[Dict Loading]ik/custom/ext_stopword.dic

8、ik使用
打开浏览器，输入
http://192.168.5.191:9200/_plugin/marvel/sense/

在左边的命令行窗口执行：
put /index

POST /index/fulltext/_mapping 
{
    "fulltext": {
             "_all": {
            "indexAnalyzer": "ik",
            "searchAnalyzer": "ik",
            "term_vector": "no",
            "store": "false"
        },
        "properties": {
            "content": {
                "type": "string",
                "store": "no",
                "term_vector": "with_positions_offsets",
                "indexAnalyzer": "ik",
                "searchAnalyzer": "ik",
                "include_in_all": "true",
                "boost": 8
            }
        }
    }
}

POST /index/fulltext/1 
{"content":"美国留给伊拉克的是个烂摊子吗"}

POST /index/fulltext/2 
{"content":"公安部：各地校车将享最高路权"}

POST /index/fulltext/3 
{"content":"中韩渔警冲突调查：韩警平均每天扣1艘中国渔船"}

POST /index/fulltext/4 
{"content":"中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首"}

验证索引数据：
GET /index/_search

vim /etc/elasticsearch/ik/custom/mydict.dic
#添加分词 
启停

尝试搜索：
POST /index/fulltext/_search  
{
    "query": {
        "match_phrase": {
            "content": "大众 启停"
        }
    },
    "highlight" : {
        "pre_tags" : ["<tag1>", "<tag2>"],
        "post_tags" : ["</tag1>", "</tag2>"],
        "fields" : {
            "content" : {}
        }
    }
}

POST /index/fulltext/_search  
{
    "query" : { 
      "match": {
            "content": {
                "query": "大众 启停",
                "slop":  1
            }
        }
    },
    "highlight" : {
        "pre_tags" : ["<tag1>", "<tag2>"],
        "post_tags" : ["</tag1>", "</tag2>"],
        "fields" : {
            "content" : {}
        }
    }
}

POST /index/_analyze?analyzer=ik&pretty=true&text=%e4%b8%ad%e5%9b%bd%e9%a9%bb%e6%b4%9b%e5%a4%a7%e4%bc%97%e5%90%af%e5%81%9c%e6%9d%89%e7%9f%b6%e9%a2%86%e4%ba%8b%e9%a6%86%e9%81%ad%e4%ba%9a%e8%a3%94%e7%94%b7%e5%ad%90%e6%9e%aa%e5%87%bb+%e5%ab%8c%e7%8a%af%e5%b7%b2%e8%87%aa%e9%a6%96


POST /index/_analyze?analyzer=ik&pretty=true&text=%e5%90%af%e5%81%9c

词典配置
vi /etc/elasticsearch/ik/IKAnalyzer.cfg.xml

测试分词?
POST /index/_analyze?analyzer=ik_smart&pretty=true&text=中华人民共和国国歌

无结果
text内容必须经过一次编码才可以
编码地址：
http://tool.chinaz.com/Tools/URLEncode.aspx

POST /index/_analyze?analyzer=ik&pretty=true&text=%e4%b8%ad%e5%8d%8e%e4%ba%ba%e6%b0%91%e5%85%b1%e5%92%8c%e5%9b%bd%e5%9b%bd%e6%ad%8c

{
   "tokens": [
      {
         "token": "中华人民共和国",
         "start_offset": 0,
         "end_offset": 7,
         "type": "CN_WORD",
         "position": 1
      },
      {
         "token": "中华人民",
         "start_offset": 0,
         "end_offset": 4,
         "type": "CN_WORD",
         "position": 2
      },
      {
         "token": "中华",
         "start_offset": 0,
         "end_offset": 2,
         "type": "CN_WORD",
         "position": 3
      },
      {
         "token": "华人",
         "start_offset": 1,
         "end_offset": 3,
         "type": "CN_WORD",
         "position": 4
      },
      {
         "token": "人民共和国",
         "start_offset": 2,
         "end_offset": 7,
         "type": "CN_WORD",
         "position": 5
      },
      {
         "token": "人民",
         "start_offset": 2,
         "end_offset": 4,
         "type": "CN_WORD",
         "position": 6
      },
      {
         "token": "共和国",
         "start_offset": 4,
         "end_offset": 7,
         "type": "CN_WORD",
         "position": 7
      },
      {
         "token": "共和",
         "start_offset": 4,
         "end_offset": 6,
         "type": "CN_WORD",
         "position": 8
      },
      {
         "token": "国",
         "start_offset": 6,
         "end_offset": 7,
         "type": "CN_CHAR",
         "position": 9
      },
      {
         "token": "国歌",
         "start_offset": 7,
         "end_offset": 9,
         "type": "CN_WORD",
         "position": 10
      }
   ]
}

自定义字典
config/ik/IKAnalyzer.cfg.xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
    <comment>IK Analyzer 扩展配置</comment>
    <!--用户可以在这里配置自己的扩展字典 -->
    <entry key="ext_dict">custom/mydict.dic;custom/single_word_low_freq.dic</entry>
     <!--用户可以在这里配置自己的扩展停止词字典-->
    <entry key="ext_stopwords">custom/ext_stopword.dic</entry>
    <!--用户可以在这里配置远程扩展字典 -->
    <entry key="remote_ext_dict">location</entry>
    <!--用户可以在这里配置远程扩展停止词字典-->
    <entry key="remote_ext_stopwords">location</entry>
</properties>

热更新 IK 分词使用方法
目前该插件支持热更新 IK 分词，通过上文在 IK 配置文件中提到的如下配置
    <!--用户可以在这里配置远程扩展字典 -->
    <entry key="remote_ext_dict">location</entry>
    <!--用户可以在这里配置远程扩展停止词字典-->
    <entry key="remote_ext_stopwords">location</entry>
其中 location 是指一个 url，比如 http://yoursite.com/getCustomDict，该请求只需满足以下两点即可完成分词热更新。

该 http 请求需要返回两个头部(header)，一个是 Last-Modified，一个是 ETag，这两者都是字符串类型，只要有一个发生变化，该插件就会去抓取新的分词进而更新词库。
该 http 请求返回的内容格式是一行一个分词，换行符用 \n 即可。
满足上面两点要求就可以实现热更新分词了，不需要重启 ES 实例。

可以将需自动更新的热词放在一个 UTF-8 编码的 .txt 文件里，放在 nginx 或其他简易 http server 下，当 .txt 文件修改时，http server 会在客户端请求该文件时自动返回相应的 Last-Modified 和 ETag。可以另外做一个工具来从业务系统提取相关词汇，并更新这个 .txt 文件。
have fun.

常见问题
1.自定义词典为什么没有生效？
请确保你的扩展词典的文本格式为 UTF8 编码

2.如何手动安装，以 1.3.0 槔？（参考：https://github.com/medcl/elasticsearch-analysis-ik/issues/46）
git clone https://github.com/medcl/elasticsearch-analysis-ik
cd elasticsearch-analysis-ik
mvn compile
mvn package
plugin --install analysis-ik --url file:///#{project_path}/elasticsearch-analysis-ik/target/releases/elasticsearch-analysis-ik-1.3.0.zip
2、自定义词库的方式，请参考 https://github.com/medcl/elasticsearch-analysis-ik
